from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StringType, DoubleType
from pymongo import MongoClient
import mysql.connector

# ‚úÖ Initialize Spark session
spark = SparkSession.builder \
    .appName("KafkaToMongoMySQL") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

# ‚úÖ Define schema with timestamp as DoubleType to accept float
schema = StructType() \
    .add("device_id", StringType()) \
    .add("temperature", DoubleType()) \
    .add("humidity", DoubleType()) \
    .add("timestamp", DoubleType())  # ‚úÖ FIXED: was LongType before

# ‚úÖ Read stream from Kafka
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:29092") \
    .option("subscribe", "iot-events") \
    .load()

# ‚úÖ Convert Kafka JSON payload to structured columns
json_df = df.selectExpr("CAST(value AS STRING)") \
    .select(from_json(col("value"), schema).alias("data")) \
    .select("data.*")

# ‚úÖ Write to MongoDB
def write_to_mongo(rows):
    try:
        client = MongoClient("mongodb://mongodb:27017")
        db = client.iot
        docs = [row.asDict() for row in rows if row.timestamp is not None]
        if docs:
            db.events.insert_many(docs)
            print(f"‚úÖ Inserted {len(docs)} rows into MongoDB")
        else:
            print("‚ö†Ô∏è No valid rows for MongoDB")
        client.close()
    except Exception as e:
        print(f"‚ùå MongoDB error: {e}")

# ‚úÖ Write to MySQL
def write_to_mysql(rows):
    try:
        conn = mysql.connector.connect(
            host="mysql",
            user="iot",
            password="iot123",
            database="iot_data"
        )
        cursor = conn.cursor()
        insert_sql = """
            INSERT INTO events (device_id, temperature, humidity, timestamp)
            VALUES (%s, %s, %s, %s)
        """
        values = [
            (row.device_id, row.temperature, row.humidity, row.timestamp)
            for row in rows if row.timestamp is not None
        ]
        if values:
            cursor.executemany(insert_sql, values)
            conn.commit()
            print(f"‚úÖ Inserted {len(values)} rows into MySQL")
        else:
            print("‚ö†Ô∏è No valid rows for MySQL")
        cursor.close()
        conn.close()
    except Exception as e:
        print(f"‚ùå MySQL error: {e}")

# ‚úÖ Process each batch
def foreach_batch_function(df, epoch_id):
    count = df.count()
    print(f"üì¶ Processing batch {epoch_id} with {count} rows")
    if count == 0:
        return
    try:
        df.persist()
        rows = list(df.toLocalIterator())
        for r in rows:
            print("üîé Row:", r.asDict())
        write_to_mongo(rows)
        write_to_mysql(rows)
        df.unpersist()
    except Exception as e:
        print(f"‚ùå Error in batch {epoch_id}: {e}")

# ‚úÖ Start structured streaming query
query = json_df.writeStream \
    .foreachBatch(foreach_batch_function) \
    .outputMode("append") \
    .start()

query.awaitTermination()
